{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4491c9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import interpolate\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torch.utils.data as Data\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch import einsum\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, LeaveOneGroupOut\n",
    "import copy\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, accuracy_score\n",
    "#from sklearn import preprocessing\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from tqdm import tqdm, trange\n",
    "from torcheeg.datasets.constants.emotion_recognition import format_region_channel_list\n",
    "import random\n",
    "import mne\n",
    "\n",
    "from torcheeg.datasets.constants.emotion_recognition.deap import DEAP_GENERAL_REGION_LIST\n",
    "from torcheeg.models import (EEGNet, FBCNet, TSCeption)\n",
    "from torcheeg.models import DGCNN, ArjunViT, STNet, FBCNet, GRU, LSTM, LGGNet, MTCNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2689cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "EEG_data=np.load('multimodal_data/data_epochs/EEG_data.npy')\n",
    "ECG_data=np.load('multimodal_data/data_epochs/ECG_data.npy')\n",
    "EDA_data=np.load('multimodal_data/data_epochs/EDA_data.npy')\n",
    "LABEL_DEC=np.load('multimodal_data/data_epochs/label_dec.npy')\n",
    "LABEL_EMO=np.load('multimodal_data/data_epochs/label_emo.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c452b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c077258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#Shuffle the data\n",
    "set_seed_everywhere(3407)\n",
    "index=np.random.permutation(EEG_data.shape[0])\n",
    "print(index)\n",
    "EEG_data=EEG_data[index]\n",
    "delete_index=[32,42,64,65,66]\n",
    "EEG_data=np.delete(EEG_data,delete_index,axis=1)\n",
    "\n",
    "ECG_data=ECG_data[index]\n",
    "EDA_data=EDA_data[index]\n",
    "dec_label=LABEL_DEC[index]\n",
    "emo_label=LABEL_EMO[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef5e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eegdata_train=EEG_data[:int(np.floor(len(EEG_data)*0.8)),:,:875]\n",
    "ecgdata_train=ECG_data[:int(np.floor(len(ECG_data)*0.8)),:7000]\n",
    "edadata_train=EDA_data[:int(np.floor(len(EDA_data)*0.8)),:7000]\n",
    "declabel_train=dec_label[:int(np.floor(len(EEG_data)*0.8))]\n",
    "emolabel_train=emo_label[:int(np.floor(len(EEG_data)*0.8))]\n",
    "\n",
    "eegdata_val=EEG_data[int(np.floor(len(EEG_data)*0.8)):np.floor(len(EEG_data)*0.9)),:,:875]\n",
    "ecgdata_val=ECG_data[int(np.floor(len(ECG_data)*0.8)):np.floor(len(ECG_data)*0.9)),:7000]\n",
    "edadata_val=EDA_data[int(np.floor(len(EDA_data)*0.8)):np.floor(len(EDA_data)*0.9)),:7000]\n",
    "emolabel_val=emo_label[int(np.floor(len(EEG_data)*0.8)):np.floor(len(EEG_data)*0.9))]\n",
    "declabel_val=dec_label[int(np.floor(len(EEG_data)*0.8)):np.floor(len(EEG_data)*0.9))]\n",
    "\n",
    "eegdata_test=EEG_data[int(np.floor(len(EEG_data)*0.9)):,:,:875]\n",
    "ecgdata_test=ECG_data[int(np.floor(len(ECG_data)*0.9)):,:7000]\n",
    "edadata_test=EDA_data[int(np.floor(len(EDA_data)*0.9)):,:7000]\n",
    "emolabel_test=emo_label[int(np.floor(len(EEG_data)*0.9)):]\n",
    "declabel_test=dec_label[int(np.floor(len(EEG_data)*0.9)):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0357cfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "np.save(\"dataset_partition/Train_eeg.npy\", eegdata_train)\n",
    "np.save(\"dataset_partition/Train_ecg.npy\", ecgdata_train)\n",
    "np.save(\"dataset_partition/Train_eda.npy\", edadata_train)\n",
    "np.save(\"dataset_partition/Train_declabel.npy\", declabel_train)\n",
    "np.save(\"dataset_partition/Train_emolabel.npy\", emolabel_train)\n",
    "\n",
    "\n",
    "np.save(\"dataset_partition/Val_eeg.npy\", eegdata_val)\n",
    "np.save(\"dataset_partition/Val_ecg.npy\", ecgdata_val)\n",
    "np.save(\"dataset_partition/Val_eda.npy\", edadata_val)\n",
    "np.save(\"dataset_partition/Val_declabel.npy\", declabel_val)\n",
    "np.save(\"dataset_partition/Val_emolabel.npy\", emolabel_val)\n",
    "\n",
    "\n",
    "np.save(\"dataset_partition/Test_eeg.npy\", eegdata_test)\n",
    "np.save(\"dataset_partition/Test_ecg.npy\", ecgdata_test)\n",
    "np.save(\"dataset_partition/Test_eda.npy\", edadata_test)\n",
    "np.save(\"dataset_partition/Test_declabel.npy\", declabel_test)\n",
    "np.save(\"dataset_partition/Test_emolabel.npy\", emolabel_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ed4838",
   "metadata": {},
   "outputs": [],
   "source": [
   
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "train_eeg_data = np.load(\"dataset_partition/Train_eeg.npy\")\n",
    "train_ecg_data = np.load(\"dataset_partition/Train_ecg.npy\")\n",
    "train_eda_data = np.load(\"dataset_partition/Train_eda.npy\")\n",
    "declabel_train = np.load(\"dataset_partition/Train_declabel.npy\")\n",
    "emolabel_train = np.load(\"dataset_partition/Train_emolabel.npy\")\n",
    "\n",
    "val_eeg_data = np.load(\"dataset_partition/Val_eeg.npy\")\n",
    "val_ecg_data = np.load(\"dataset_partition/Val_ecg.npy\")\n",
    "val_eda_data = np.load(\"dataset_partition/Val_eda.npy\")\n",
    "val_declabel = np.load(\"dataset_partition/Val_declabel.npy\")\n",
    "val_emolabel = np.load(\"dataset_partition/Val_emolabel.npy\")\n",
    "\n",
    "test_eeg_data = np.load(\"dataset_partition/Test_eeg.npy\")\n",
    "test_ecg_data = np.load(\"dataset_partition/Test_ecg.npy\")\n",
    "test_eda_data = np.load(\"dataset_partition/Test_eda.npy\")\n",
    "test_declabel = np.load(\"dataset_partition/Test_declabel.npy\")\n",
    "test_emolabel = np.load(\"dataset_partition/Test_emolabel.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "406964f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "train_eeg_data = torch.from_numpy(train_eeg_data).float().reshape(-1,1,62,875)\n",
    "train_ecg_data = torch.from_numpy(train_ecg_data).float()\n",
    "train_eda_data = torch.from_numpy(train_eda_data).float()\n",
    "train_dec_label = torch.from_numpy(declabel_train).long()\n",
    "train_emo_label = torch.from_numpy(emolabel_train).long()\n",
    "\n",
    "val_eeg_data = torch.from_numpy(val_eeg_data).float().reshape(-1, 1, 62, 875)\n",
    "val_ecg_data = torch.from_numpy(val_ecg_data).float()\n",
    "val_eda_data = torch.from_numpy(val_eda_data).float()\n",
    "val_dec_label = torch.from_numpy(val_declabel).long()\n",
    "val_emo_label = torch.from_numpy(val_emolabel).long()\n",
    "\n",
    "test_eeg_data = torch.from_numpy(test_eeg_data).float().reshape(-1,1,62,875)\n",
    "test_ecg_data = torch.from_numpy(test_ecg_data).float()\n",
    "test_eda_data = torch.from_numpy(test_eda_data).float()\n",
    "test_dec_label = torch.from_numpy(test_declabel).long()\n",
    "test_emo_label= torch.from_numpy(test_emolabel).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "990be245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1698, 1, 62, 875]),\n",
       " torch.Size([1698, 7000]),\n",
       " torch.Size([1698, 7000]),\n",
       " torch.Size([1698]),\n",
       " torch.Size([1698]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_eeg_data.shape,train_ecg_data.shape, train_eda_data.shape, train_dec_label.shape, train_emo_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae143a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downsampling of ecg and eda\n",
    "window_size = 8\n",
    "\n",
    "avg_pool = torch.nn.AvgPool1d(kernel_size=window_size, stride=window_size)\n",
    "\n",
    "train_ecg_data = avg_pool(train_ecg_data)\n",
    "train_eda_data = avg_pool(train_eda_data)\n",
    "\n",
    "val_ecg_data = avg_pool(val_ecg_data)\n",
    "val_eda_data = avg_pool(val_eda_data)\n",
    "\n",
    "test_ecg_data = avg_pool(test_ecg_data)\n",
    "test_eda_data = avg_pool(test_eda_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b34a1f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1698, 1, 62, 875]),\n",
       " torch.Size([1698, 875]),\n",
       " torch.Size([1698, 875]),\n",
       " torch.Size([1698]),\n",
       " torch.Size([1698]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_eeg_data.shape,train_ecg_data.shape, train_eda_data.shape, train_dec_label.shape, train_emo_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07bc48e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([212, 1, 62, 875]),\n",
       " torch.Size([212, 875]),\n",
       " torch.Size([212, 875]),\n",
       " torch.Size([212]),\n",
       " torch.Size([212]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_eeg_data.shape,val_ecg_data.shape, val_eda_data.shape, val_dec_label.shape, val_emo_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b4f17b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([213, 1, 62, 875]),\n",
       " torch.Size([213, 875]),\n",
       " torch.Size([213, 875]),\n",
       " torch.Size([213]),\n",
       " torch.Size([213]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_eeg_data.shape,test_ecg_data.shape, test_eda_data.shape, test_dec_label.shape, test_emo_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354ccb4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##asf and mtf features of ecg and eda in train set\n",
    "from pyts.image import GramianAngularField, MarkovTransitionField\n",
    "\n",
    "gasf = GramianAngularField(image_size=64, method='summation')\n",
    "ecg_gasf_train = gasf.fit_transform(train_ecg_data)\n",
    "eda_gasf_train = gasf.fit_transform(train_eda_data)\n",
    "mtf = MarkovTransitionField(image_size=64)\n",
    "ecg_mtf_train = mtf.fit_transform(train_ecg_data)\n",
    "eda_mtf_train = mtf.fit_transform(train_eda_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e1345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gasf and mtf features of ecg and eda in val set\n",
    "from pyts.image import GramianAngularField, MarkovTransitionField\n",
    "\n",
    "gasf = GramianAngularField(image_size=64, method='summation')\n",
    "ecg_gasf_val = gasf.fit_transform(val_ecg_data)\n",
    "eda_gasf_val = gasf.fit_transform(val_eda_data)\n",
    "mtf = MarkovTransitionField(image_size=64)\n",
    "ecg_mtf_val = mtf.fit_transform(val_ecg_data)\n",
    "eda_mtf_val = mtf.fit_transform(val_eda_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6840b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gasf and mtf features of ecg and eda in test set\n",
    "from pyts.image import GramianAngularField, MarkovTransitionField\n",
    "\n",
    "gasf = GramianAngularField(image_size=64, method='summation')\n",
    "ecg_gasf_test = gasf.fit_transform(test_ecg_data)\n",
    "eda_gasf_test = gasf.fit_transform(test_eda_data)\n",
    "mtf = MarkovTransitionField(image_size=64)\n",
    "ecg_mtf_test = mtf.fit_transform(test_ecg_data)\n",
    "eda_mtf_test = mtf.fit_transform(test_eda_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dde6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RP feature of ecg and eda in train set\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "def recurrence_plot(s, eps=None, steps=None):\n",
    "    if eps is None: eps = 0.1\n",
    "    if steps is None: steps = 10\n",
    "    d = pairwise_distances(s[:, None])\n",
    "    d = d / eps\n",
    "    d[d > steps] = steps\n",
    "    return d / 5.0 - 1\n",
    "\n",
    "def compress_image(image, target_size):\n",
    "    compressed_image = cv2.resize(image, (target_size, target_size), interpolation=cv2.INTER_LINEAR)\n",
    "    return compressed_image\n",
    "\n",
    "\n",
    "ecg_rp_train = []\n",
    "eda_rp_train = []\n",
    "\n",
    "## target image size\n",
    "target_size = 64\n",
    "\n",
    "for i in range(len(train_ecg_data)):\n",
    "    result = recurrence_plot(train_ecg_data[i], steps=10)\n",
    "    compressed_result = compress_image(result, target_size)\n",
    "    ecg_rp_train.append(compressed_result)\n",
    "\n",
    "for i in range(len(train_eda_data)):\n",
    "    result = recurrence_plot(train_eda_data[i], steps=10)\n",
    "    compressed_result = compress_image(result, target_size)\n",
    "    eda_rp_train.append(compressed_result)\n",
    "\n",
    "ecg_rp_train = np.array(ecg_rp_train)\n",
    "eda_rp_train = np.array(eda_rp_train)\n",
    "\n",
    "print(ecg_rp_train.shape)\n",
    "print(eda_rp_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b941db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RP feature of ecg and eda in val set\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "def recurrence_plot(s, eps=None, steps=None):\n",
    "    if eps is None: eps = 0.1\n",
    "    if steps is None: steps = 10\n",
    "    d = pairwise_distances(s[:, None])\n",
    "    d = d / eps\n",
    "    d[d > steps] = steps\n",
    "    return d / 5.0 - 1\n",
    "\n",
    "def compress_image(image, target_size):\n",
    "\n",
    "    compressed_image = cv2.resize(image, (target_size, target_size), interpolation=cv2.INTER_LINEAR)\n",
    "    return compressed_image\n",
    "\n",
    "ecg_rp_val = []\n",
    "eda_rp_val = []\n",
    "\n",
    "# target image size\n",
    "target_size = 64\n",
    "\n",
    "for i in range(len(val_ecg_data)):\n",
    "    result = recurrence_plot(val_ecg_data[i], steps=10)\n",
    "    compressed_result = compress_image(result, target_size)\n",
    "    ecg_rp_val.append(compressed_result)\n",
    "\n",
    "for i in range(len(val_eda_data)):\n",
    "    result = recurrence_plot(val_eda_data[i], steps=10)\n",
    "    compressed_result = compress_image(result, target_size)\n",
    "    eda_rp_val.append(compressed_result)\n",
    "\n",
    "ecg_rp_val = np.array(ecg_rp_val)\n",
    "eda_rp_val = np.array(eda_rp_val)\n",
    "\n",
    "print(ecg_rp_val.shape)\n",
    "print(eda_rp_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e230ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RP feature of ecg and eda in test set\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "def recurrence_plot(s, eps=None, steps=None):\n",
    "    if eps is None: eps = 0.1\n",
    "    if steps is None: steps = 10\n",
    "    d = pairwise_distances(s[:, None])\n",
    "    d = d / eps\n",
    "    d[d > steps] = steps\n",
    "    return d / 5.0 - 1\n",
    "\n",
    "def compress_image(image, target_size):\n",
    "    compressed_image = cv2.resize(image, (target_size, target_size), interpolation=cv2.INTER_LINEAR)\n",
    "    return compressed_image\n",
    "\n",
    "ecg_rp_test = []\n",
    "eda_rp_test = []\n",
    "\n",
    "# target image size\n",
    "target_size = 64\n",
    "\n",
    "for i in range(len(test_ecg_data)):\n",
    "    result = recurrence_plot(test_ecg_data[i], steps=10)\n",
    "    compressed_result = compress_image(result, target_size)\n",
    "    ecg_rp_test.append(compressed_result)\n",
    "\n",
    "for i in range(len(test_eda_data)):\n",
    "    result = recurrence_plot(test_eda_data[i], steps=10)\n",
    "    compressed_result = compress_image(result, target_size)\n",
    "    eda_rp_test.append(compressed_result)\n",
    "\n",
    "ecg_rp_test = np.array(ecg_rp_test)\n",
    "eda_rp_test = np.array(eda_rp_test)\n",
    "\n",
    "print(ecg_rp_test.shape)\n",
    "print(eda_rp_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcc8882",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot 2D-image features of ecg\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "\n",
    "axs[0].imshow(ecg_gasf_train[1], cmap='gray', origin='lower')\n",
    "axs[0].set_title('GASF Image')\n",
    "\n",
    "\n",
    "axs[1].imshow(ecg_mtf_train[1], cmap='gray', origin='lower')\n",
    "axs[1].set_title('MTF Image')\n",
    "\n",
    "\n",
    "axs[2].imshow(ecg_rp_train[1], cmap='gray', origin='lower')\n",
    "axs[2].set_title('RP Image')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc962a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot 2D-image features of eda\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "\n",
    "axs[0].imshow(eda_gasf_train[1], cmap='gray', origin='lower')\n",
    "axs[0].set_title('GASF Image')\n",
    "\n",
    "\n",
    "\n",
    "axs[1].imshow(eda_mtf_train[1], cmap='gray', origin='lower')\n",
    "axs[1].set_title('MTF Image')\n",
    "\n",
    "\n",
    "axs[2].imshow(eda_rp_train[1], cmap='gray', origin='lower')\n",
    "axs[2].set_title('RP Image')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d74cab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "train_eda_image = torch.from_numpy(train_eda_image)\n",
    "val_eda_image = torch.from_numpy(val_eda_image)\n",
    "test_eda_image = torch.from_numpy(test_eda_image)\n",
    "\n",
    "train_ecg_image = torch.from_numpy(train_ecg_image)\n",
    "val_ecg_image = torch.from_numpy(val_ecg_image)\n",
    "test_ecg_image = torch.from_numpy(test_ecg_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64093ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details open>\n",
       "    <summary><strong>General</strong></summary>\n",
       "    <table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "        <tr>\n",
       "            <th>Measurement date</th>\n",
       "            \n",
       "            <td>Unknown</td>\n",
       "            \n",
       "        </tr>\n",
       "        <tr>\n",
       "            <th>Experimenter</th>\n",
       "            \n",
       "            <td>Unknown</td>\n",
       "            \n",
       "        </tr>\n",
       "        <tr>\n",
       "            <th>Participant</th>\n",
       "            \n",
       "            <td>Unknown</td>\n",
       "            \n",
       "        </tr>\n",
       "    </table>\n",
       "    </details>\n",
       "    <details open>\n",
       "        <summary><strong>Channels</strong></summary>\n",
       "        <table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "            <tr>\n",
       "                <th>Digitized points</th>\n",
       "                \n",
       "                <td>67 points</td>\n",
       "                \n",
       "            </tr>\n",
       "            <tr>\n",
       "                <th>Good channels</th>\n",
       "                <td>62 EEG</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <th>Bad channels</th>\n",
       "                <td>None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <th>EOG channels</th>\n",
       "                <td>Not available</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <th>ECG channels</th>\n",
       "                <td>Not available</td>\n",
       "            </tr>\n",
       "        </table>\n",
       "        </details>\n",
       "        <details open>\n",
       "            <summary><strong>Data</strong></summary>\n",
       "            <table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "                \n",
       "                <tr>\n",
       "                    <th>Sampling frequency</th>\n",
       "                    <td>1000.00 Hz</td>\n",
       "                </tr>\n",
       "                \n",
       "                \n",
       "                <tr>\n",
       "                    <th>Highpass</th>\n",
       "                    <td>0.00 Hz</td>\n",
       "                </tr>\n",
       "                \n",
       "                \n",
       "                <tr>\n",
       "                    <th>Lowpass</th>\n",
       "                    <td>500.00 Hz</td>\n",
       "                </tr>\n",
       "                \n",
       "                \n",
       "                \n",
       "                <tr>\n",
       "                    <th>Filenames</th>\n",
       "                    <td>01102201.fdt</td>\n",
       "                </tr>\n",
       "                \n",
       "                \n",
       "                <tr>\n",
       "                    <th>Duration</th>\n",
       "                    <td>00:10:60 (HH:MM:SS)</td>\n",
       "                </tr>\n",
       "                \n",
       "            </table>\n",
       "            </details>"
      ],
      "text/plain": [
       "<RawEEGLAB | 01102201.fdt, 62 x 659300 (659.3 s), ~312.0 MB, data loaded>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mne\n",
    "eeg_data_now = mne.io.read_raw_eeglab('/data/zhaoyanyan/notebook_code/2024-4-22备份/model/Multimodal-Transformer-master/src/01102201.set', preload=True)\n",
    "channels_to_remove = ['M1', 'M2','HEO','VEO','Trigger']\n",
    "\n",
    "eeg_data_now.pick_channels(ch_names=[ch for ch in eeg_data_now.ch_names if ch not in channels_to_remove])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8357bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheeg.datasets.constants.emotion_recognition import format_region_channel_list\n",
    "'''\n",
    "To Grid\n",
    "'''\n",
    "LOCATION_LIST = [\n",
    "    [ '-', '-', '-', 'FP1', 'FPZ', 'FP2', '-', '-', '-'],\n",
    "    [ '-', '-', '-', 'AF3', '-', 'AF4', '-', '-', '-'],\n",
    "    [ 'F7', 'F5', 'F3', 'F1', 'FZ', 'F2', 'F4', 'F6', 'F8'],\n",
    "    [ 'FT7', 'FC5', 'FC3', 'FC1', 'FCZ', 'FC2', 'FC4', 'FC6', 'FT8'],\n",
    "    [ 'T7', 'C5', 'C3', 'C1', 'CZ', 'C2', 'C4', 'C6', 'T8'],\n",
    "    [ 'TP7', 'CP5', 'CP3', 'CP1', 'CPZ', 'CP2', 'CP4', 'CP6', 'TP8'],\n",
    "    [ 'P7', 'P5', 'P3', 'P1', 'PZ', 'P2', 'P4', 'P6', 'P8'],\n",
    "    [ '-', 'PO7', 'PO5', 'PO3', 'POZ', 'PO4', 'PO6', 'PO8', '-'],\n",
    "    [ '-', '-', 'CB1', 'O1', 'OZ', 'O2', 'CB2', '-', '-' ]\n",
    "]\n",
    "HEMISPHERE_LIST = [\n",
    "    ['F7', 'F5', 'F3', 'F1'], \n",
    "    ['F2', 'F4', 'F6', 'F8'],\n",
    "    [ 'FC5', 'FC3', 'FC1'],\n",
    "    ['FC2', 'FC4', 'FC6'], \n",
    "    ['FP1',  'AF3'], \n",
    "    ['FP2', 'AF4', ],\n",
    "    ['FPZ', 'FZ', 'FCZ', 'CZ', 'CPZ', 'PZ', 'POZ', 'OZ'],\n",
    "    ['C5', 'C3', 'C1'], ['C2', 'C4', 'C6'], ['CP5', 'CP3', 'CP1'],\n",
    "    ['CP2', 'CP4', 'CP6'], \n",
    "    ['P7', 'P5', 'P3', 'P1'], \n",
    "    ['P2', 'P4', 'P6', 'P8'],\n",
    "    ['PO7','PO5', 'PO3', 'O1', 'CB1'], \n",
    "    ['PO4', 'PO6', 'PO8', 'O2', 'CB2'], \n",
    "    ['FT7','T7', 'TP7'],\n",
    "    ['FT8','T8', 'TP8' ]\n",
    "]\n",
    "CHANNELS_LIST = eeg_data_now.ch_names\n",
    "\n",
    "'''\n",
    "local-global structure\n",
    "'''\n",
    "GENERAL_REGION_LIST = format_region_channel_list(CHANNELS_LIST, LOCATION_LIST)     #按行分\n",
    "\n",
    "HEMISPHERE_REGION_LIST = format_region_channel_list(CHANNELS_LIST, HEMISPHERE_LIST)            #按区域分\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10d422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_size):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(in_size*2, 64),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(64, in_size*2),\n",
    "        )\n",
    "        self.Linear1= nn.Linear(in_size*2,in_size)\n",
    "    def forward(self, x):\n",
    "        x= x + self.block(x)\n",
    "        x= self.Linear1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class graph11_new(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size, hidden = 50, dropout=0.5):\n",
    "\n",
    "        super(graph11_new, self).__init__()\n",
    "        self.norm = nn.BatchNorm1d(in_size)\n",
    "        self.norm2 = nn.BatchNorm1d(in_size*3)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "      #  self.graph = nn.Linear(in_size*2, in_size)\n",
    "\n",
    "        self.graph_fusion = nn.Sequential(\n",
    "            ResidualBlock(in_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "        self.graph_fusion2 = nn.Sequential(\n",
    "            ResidualBlock(in_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.attention = nn.Linear(in_size, 1)\n",
    "        self.linear_1 = nn.Linear(in_size*3, hidden)\n",
    "        self.linear_2 = nn.Linear(hidden, hidden)\n",
    "        self.hidden = hidden\n",
    "        self.in_size = in_size\n",
    "\n",
    "      #  self.u1 = Parameter(torch.Tensor(in_size, in_size).cuda())\n",
    "     #   xavier_normal(self.u1)\n",
    "\n",
    "    def forward(self, a1,v1,l1):\n",
    "       \n",
    "        ###################### unimodal layer  ##########################\n",
    "        sa = F.sigmoid(self.attention(a1))\n",
    "        sv = F.sigmoid(self.attention(v1))\n",
    "        sl = F.sigmoid(self.attention(l1))\n",
    "\n",
    "        total_weights = torch.cat([sa, sv, sl],1)\n",
    "\n",
    "        unimodal_a = (sa.expand(a1.size(0),self.in_size))\n",
    "        unimodal_v = (sv.expand(a1.size(0),self.in_size))\n",
    "        unimodal_l = (sl.expand(a1.size(0),self.in_size))\n",
    "        sa = sa.squeeze()\n",
    "        sl = sl.squeeze()\n",
    "        sv = sv.squeeze()\n",
    "        unimodal = (unimodal_a * a1 + unimodal_v * v1 + unimodal_l * l1)/3\n",
    "\n",
    "        ##################### bimodal layer ############################\n",
    "        a = F.softmax(a1, 1)\n",
    "        v = F.softmax(v1, 1)\n",
    "        l = F.softmax(l1, 1)\n",
    "        sav = (1/(torch.matmul(a.unsqueeze(1), v.unsqueeze(2)).squeeze() +0.5) *(sa+sv))\n",
    "        sal = (1/(torch.matmul(a.unsqueeze(1), l.unsqueeze(2)).squeeze() +0.5) *(sa+sl))\n",
    "        svl = (1/(torch.matmul(v.unsqueeze(1), l.unsqueeze(2)).squeeze() +0.5) *(sl+sv))\n",
    "     #   print('sav',sav.shape)\n",
    "        normalize = torch.cat([sav.unsqueeze(1), sal.unsqueeze(1), svl.unsqueeze(1)],1)\n",
    "        normalize = F.softmax(normalize,1)\n",
    "        total_weights = torch.cat([total_weights,normalize],1)\n",
    "    #    print('normalize',normalize.shape)\n",
    "     #   print((normalize[:,0].unsqueeze(1).expand(a.size(0),self.in_size)).shape,'shape')\n",
    "        a_v = F.elu((normalize[:,0].unsqueeze(1).expand(a.size(0), self.in_size)) * self.graph_fusion(torch.cat([a1,v1],1)))\n",
    "        a_l = F.elu((normalize[:,1].unsqueeze(1).expand(a.size(0), self.in_size)) * self.graph_fusion(torch.cat([a1,l1],1)))\n",
    "        v_l = F.elu((normalize[:,2].unsqueeze(1).expand(a.size(0), self.in_size)) * self.graph_fusion(torch.cat([v1,l1],1)))\n",
    "        bimodal = (a_v + a_l + v_l)\n",
    "    \n",
    "        ###################### trimodal layer ####################################\n",
    "        a_v2 = F.softmax(self.graph_fusion(torch.cat([a1,v1],1)), 1)\n",
    "        a_l2 = F.softmax(self.graph_fusion(torch.cat([a1,l1],1)), 1)\n",
    "        v_l2 = F.softmax(self.graph_fusion(torch.cat([v1,l1],1)), 1)\n",
    "        savvl = (1/(torch.matmul(a_v2.unsqueeze(1), v_l2.unsqueeze(2)).squeeze() +0.5) *(sav+svl))\n",
    "        saavl = (1/(torch.matmul(a_v2.unsqueeze(1), a_l2.unsqueeze(2)).squeeze() +0.5) *(sav+sal))\n",
    "        savll = (1/(torch.matmul(a_l2.unsqueeze(1), v_l2.unsqueeze(2)).squeeze() +0.5) *(sal+svl))\n",
    "        savl = (1/(torch.matmul(a_v2.unsqueeze(1), l.unsqueeze(2)).squeeze() +0.5) *(sav+sl))\n",
    "        salv = (1/(torch.matmul(a_l2.unsqueeze(1), v.unsqueeze(2)).squeeze() +0.5) *(sal+sv))\n",
    "        svla = (1/(torch.matmul(v_l2.unsqueeze(1), a.unsqueeze(2)).squeeze() +0.5) *(sa+svl))\n",
    "\n",
    "        normalize2 = torch.cat([savvl.unsqueeze(1), saavl.unsqueeze(1), savll.unsqueeze(1), savl.unsqueeze(1), salv.unsqueeze(1), svla.unsqueeze(1)],1)\n",
    "        normalize2 = F.softmax(normalize2,1)\n",
    "        total_weights = torch.cat([total_weights,normalize2],1)\n",
    "       # print((normalize2[:,0].unsqueeze(1).expand(a.size(0),self.in_size)).shape,'shape')\n",
    "        avvl = F.elu((normalize2[:,0].unsqueeze(1).expand(a.size(0),self.in_size)) * self.graph_fusion2(torch.cat([a_v,v_l],1)))\n",
    "        aavl = F.elu((normalize2[:,1].unsqueeze(1).expand(a.size(0),self.in_size)) * self.graph_fusion2(torch.cat([a_v,a_l],1)))\n",
    "        avll = F.elu((normalize2[:,2].unsqueeze(1).expand(a.size(0),self.in_size)) * self.graph_fusion2(torch.cat([v_l,a_l],1)))\n",
    "        avl = F.elu((normalize2[:,3].unsqueeze(1).expand(a.size(0),self.in_size)) * self.graph_fusion2(torch.cat([a_v,l1],1)))\n",
    "        alv = F.elu((normalize2[:,4].unsqueeze(1).expand(a.size(0),self.in_size)) * self.graph_fusion2(torch.cat([a_l,v1],1)))\n",
    "        vla = F.elu((normalize2[:,5].unsqueeze(1).expand(a.size(0),self.in_size)) * self.graph_fusion2(torch.cat([v_l,a1],1)))\n",
    "        trimodal = (avvl + aavl + avll + avl + alv + vla)\n",
    "        fusion = torch.cat([unimodal,bimodal],1)\n",
    "        fusion = torch.cat([fusion,trimodal],1)        \n",
    "        fusion = self.norm2(fusion)\n",
    "     #   fusion = self.drop(fusion)\n",
    "        y_1 = F.tanh(self.linear_1(fusion))\n",
    "        y_1 = F.tanh(self.linear_2(y_1))\n",
    "\n",
    "        return y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daa92e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.parameter import Parameter\n",
    "from torchvision.models import resnet18\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channelsnel, ratio=2):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_channelsnel,\n",
    "                             in_channelsnel // ratio,\n",
    "                             1,\n",
    "                             bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(in_channelsnel // ratio,\n",
    "                             in_channelsnel,\n",
    "                             1,\n",
    "                             bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_pool_out = self.avg_pool(x)\n",
    "        max_pool_out = self.max_pool(x)\n",
    "        avg_pool_out = self.fc2(self.relu1(self.fc1(avg_pool_out)))\n",
    "        max_pool_out = self.fc2(self.relu1(self.fc1(max_pool_out)))\n",
    "        out = max_pool_out + avg_pool_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        assert kernel_size in (3, 7), 'Kernel size must be 3 or 7.'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_pool_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        avg_pool_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        out = torch.cat([avg_pool_out, max_pool_out], dim=1)\n",
    "        out = self.conv1(out)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "class CBAMBlock(nn.Module):\n",
    "    def __init__(self, in_channelsnel, ratio=2, kernel_size=7):\n",
    "        super(CBAMBlock, self).__init__()\n",
    "        self.cha_att = ChannelAttention(in_channelsnel, ratio=ratio)\n",
    "        self.spa_att = SpatialAttention(kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x * self.cha_att(x)\n",
    "        out = out * self.spa_att(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PowerLayer(nn.Module):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(PowerLayer, self).__init__()\n",
    "        self.pooling = nn.AvgPool2d(kernel_size=(1, kernel_size),\n",
    "                                    stride=(1, stride))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.log(self.pooling(x.pow(2)))\n",
    "\n",
    "\n",
    "class Aggregator():\n",
    "    def __init__(self, region_list):\n",
    "        self.region_list = region_list\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = []\n",
    "        for region_index in range(len(self.region_list)):\n",
    "            region_x = x[:, self.region_list[region_index], :]\n",
    "            aggr_region_x = torch.mean(region_x, dim=1)\n",
    "            output.append(aggr_region_x)\n",
    "        return torch.stack(output, dim=1)\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    def __init__(self, in_channels, out_channels, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.weight = Parameter(torch.FloatTensor(in_channels, out_channels))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(\n",
    "                torch.zeros((1, 1, out_channels), dtype=torch.float32))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        nn.init.xavier_uniform_(self.weight, gain=1.414)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        output = torch.matmul(x, self.weight) - self.bias\n",
    "        output = F.relu(torch.matmul(adj, output))\n",
    "        return output\n",
    "\n",
    "\n",
    "class GRF_MMPS(nn.Module):\n",
    "    def __init__(self,\n",
    "                 region_list,\n",
    "                 in_channels: int = 1,\n",
    "                 num_electrodes: int = 32,\n",
    "                 chunk_size: int = 128,\n",
    "                 sampling_rate: int = 128,\n",
    "                 num_T: int = 64,\n",
    "                 hid_channels: int = 32,\n",
    "                 dropout: float = 0.5,\n",
    "                 pool_kernel_size: int = 16,\n",
    "                 pool_stride: int = 4,\n",
    "                 num_classes_dec: int = 2,\n",
    "                num_classes_emo:int = 4):\n",
    "        super(GRF-MMPS, self).__init__()\n",
    "        self.region_list = region_list\n",
    "        self.inception_window = [0.5, 0.25, 0.125]\n",
    "\n",
    "        self.num_classes_dec = num_classes_dec\n",
    "        self.num_classes_emo= num_classes_emo\n",
    "        self.in_channels = in_channels\n",
    "        self.num_electrodes = num_electrodes\n",
    "        self.chunk_size = chunk_size\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.num_T = num_T\n",
    "        self.hid_channels = hid_channels\n",
    "        self.dropout = dropout\n",
    "        self.pool_kernel_size = pool_kernel_size\n",
    "        self.pool_stride = pool_stride\n",
    "        self.in_channels = in_channels\n",
    "        self.num_electrodes = num_electrodes\n",
    "        self.graph11_new=graph11_new(512,50,0.5)\n",
    "\n",
    "        self.t_block1 = self.temporal_block(\n",
    "            self.in_channels, self.num_T,\n",
    "            (1, int(self.inception_window[0] * self.sampling_rate)),\n",
    "            self.pool_kernel_size, self.pool_stride)\n",
    "        self.t_block2 = self.temporal_block(\n",
    "            self.in_channels, self.num_T,\n",
    "            (1, int(self.inception_window[1] * self.sampling_rate)),\n",
    "            self.pool_kernel_size, self.pool_stride)\n",
    "        self.t_block3 = self.temporal_block(\n",
    "            self.in_channels, self.num_T,\n",
    "            (1, int(self.inception_window[2] * self.sampling_rate)),\n",
    "            self.pool_kernel_size, self.pool_stride)\n",
    "\n",
    "        self.bn_t1 = nn.BatchNorm2d(self.num_T)\n",
    "        self.bn_t2 = nn.BatchNorm2d(self.num_T)\n",
    "\n",
    "        self.cbam = CBAMBlock(num_electrodes)\n",
    "\n",
    "        self.conv1x1 = nn.Sequential(\n",
    "            nn.Conv2d(num_T, num_T, kernel_size=(1, 1), stride=(1, 1)),\n",
    "            nn.LeakyReLU(), nn.AvgPool2d((1, 2)))\n",
    "\n",
    "        self.avg_pool = nn.AvgPool2d((1, 2))\n",
    "\n",
    "        feature_dim = self.feature_dim\n",
    "        self.local_filter_weight = nn.Parameter(torch.FloatTensor(\n",
    "            self.num_electrodes, feature_dim),\n",
    "                                                requires_grad=True)\n",
    "        self.local_filter_bias = nn.Parameter(torch.zeros(\n",
    "            (1, self.num_electrodes, 1), dtype=torch.float32),\n",
    "                                              requires_grad=True)\n",
    "\n",
    "        self.aggregate = Aggregator(self.region_list)\n",
    "        num_region = len(self.region_list)\n",
    "\n",
    "        self.global_adj = nn.Parameter(torch.FloatTensor(\n",
    "            num_region, num_region),\n",
    "                                       requires_grad=True)\n",
    "\n",
    "        self.bn_g1 = nn.BatchNorm1d(num_region)\n",
    "        self.bn_g2 = nn.BatchNorm1d(num_region)\n",
    "\n",
    "        self.gcn = GraphConvolution(feature_dim, hid_channels)\n",
    "        \n",
    "        \n",
    "\n",
    "        self.fc_dec = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(50, num_classes_dec))\n",
    "        \n",
    "        self.fc_emo = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(50, num_classes_emo))\n",
    "\n",
    "        nn.init.xavier_uniform_(self.local_filter_weight)\n",
    "        nn.init.xavier_uniform_(self.global_adj)\n",
    "        \n",
    "        self.fc_512 = nn.Linear(int(num_region * hid_channels), 512)\n",
    "        \n",
    "        self.bn_512 = nn.BatchNorm1d(512)\n",
    "        \n",
    "        #ecg\n",
    "        self.resnet18_ecg=resnet18(pretrained=True)\n",
    "        in_features_ecg = self.resnet18_ecg.fc.in_features\n",
    "        \n",
    "        self.resnet18_ecg.fc = nn.Identity()\n",
    "        self.ecg_fc = nn.Linear(in_features_ecg, 32)\n",
    "        self.bn_ecg = nn.BatchNorm1d(512)\n",
    "        self.transform_ecg = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        #eda\n",
    "        self.resnet18_eda=resnet18(pretrained=True)\n",
    "        in_features_eda = self.resnet18_eda.fc.in_features\n",
    "        self.resnet18_eda.fc = nn.Identity()\n",
    "        self.eda_fc = nn.Linear(in_features_eda, 32)\n",
    "        self.bn_eda = nn.BatchNorm1d(512)\n",
    "        self.transform_eda = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def temporal_block(self, in_channels, out_channels, kernel_size,\n",
    "                       pool_kernel_size, pool_stride):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels,\n",
    "                      out_channels,\n",
    "                      kernel_size=kernel_size,\n",
    "                      stride=(1, 1)),\n",
    "            PowerLayer(kernel_size=pool_kernel_size, stride=pool_stride))\n",
    "\n",
    "    def forward(self, x_eeg,x_ecg,x_eda):\n",
    "        t1 = self.t_block1(x_eeg)\n",
    "        t2 = self.t_block2(x_eeg)\n",
    "        t3 = self.t_block3(x_eeg)\n",
    "        x_eeg = torch.cat((t1, t2, t3), dim=-1)\n",
    "\n",
    "        x_eeg = self.bn_t1(x_eeg)\n",
    "\n",
    "        x_eeg = x_eeg.permute(0, 2, 1, 3)\n",
    "        x_eeg = self.cbam(x_eeg)\n",
    "        x_eeg = self.avg_pool(x_eeg)\n",
    "\n",
    "        x_eeg = x_eeg.flatten(start_dim=2)\n",
    "#         print(x_eeg.shape)\n",
    "        x_eeg = self.local_filter(x_eeg)\n",
    "        x_eeg = self.aggregate.forward(x_eeg)\n",
    "        adj = self.get_adj(x_eeg)\n",
    "        x_eeg = self.bn_g1(x_eeg)\n",
    "\n",
    "        x_eeg = self.gcn(x_eeg, adj)\n",
    "        x_eeg = self.bn_g2(x_eeg)\n",
    "        x_eeg = x_eeg.view(x_eeg.shape[0], -1)\n",
    "        x_eeg = self.fc_512(x_eeg)\n",
    "#         x_eeg = self.bn_512(x_eeg)\n",
    "        \n",
    "        #ecg、eda feature extraction\n",
    "#         x_ecg = x_ecg.permute(0, 3, 1, 2)\n",
    "        x_ecg = x_ecg.cpu().numpy()\n",
    "        ecg_input_pil = [Image.fromarray((img * 255).astype(np.uint8)) for img in x_ecg]  # 转换为 PIL 图像\n",
    "        ecg_input_transformed = [self.transform_ecg(img) for img in ecg_input_pil]\n",
    "        x_ecg = torch.stack(ecg_input_transformed).to('cuda:0')  # 转换为 batch tensor\n",
    "#         x_ecg = self.transform_ecg(x_ecg)\n",
    "        x_ecg = self.resnet18_ecg(x_ecg)\n",
    "#         x_ecg = self.ecg_fc(x_ecg)\n",
    "#         x_ecg = self.bn_ecg(x_ecg)\n",
    "        \n",
    "#         x_eda = x_eda.permute(0, 3, 1, 2)\n",
    "#         x_eda = self.transform_eda(x_eda)\n",
    "        x_eda = x_eda.cpu().numpy()\n",
    "        eda_input_pil = [Image.fromarray((img * 255).astype(np.uint8)) for img in x_eda]  # 转换为 PIL 图像\n",
    "        eda_input_transformed = [self.transform_eda(img) for img in eda_input_pil]\n",
    "        x_eda = torch.stack(eda_input_transformed).to('cuda:0')  # 转换为 batch tensor\n",
    "        x_eda = self.resnet18_eda(x_eda)\n",
    "#         x_eda = self.eda_fc(x_eda)\n",
    "#         x_eda = self.bn_eda(x_eda)\n",
    "        \n",
    "        x_multi = self.graph11_new(x_eeg,x_ecg,x_eda)\n",
    "        \n",
    "        x_multi1 = self.fc_dec(x_multi)\n",
    "        x_multi2= self.fc_emo(x_multi)\n",
    "        return  x_multi1, x_multi2,x_multi\n",
    "\n",
    "    @property\n",
    "    def feature_dim(self):\n",
    "        mock_eeg = torch.randn(\n",
    "            (1, self.in_channels, self.num_electrodes, self.chunk_size))\n",
    "\n",
    "        t1 = self.t_block1(mock_eeg)\n",
    "        t2 = self.t_block2(mock_eeg)\n",
    "        t3 = self.t_block3(mock_eeg)\n",
    "        mock_eeg = torch.cat((t1, t2, t3), dim=-1)\n",
    "\n",
    "        mock_eeg = self.bn_t1(mock_eeg)\n",
    "        mock_eeg = self.conv1x1(mock_eeg)\n",
    "        mock_eeg = self.bn_t2(mock_eeg)\n",
    "        mock_eeg = mock_eeg.permute(0, 2, 1, 3)\n",
    "        mock_eeg = mock_eeg.flatten(start_dim=2)\n",
    "        return mock_eeg.shape[-1]\n",
    "\n",
    "    def local_filter(self, x):\n",
    "        w = self.local_filter_weight.unsqueeze(0).repeat(x.shape[0], 1, 1)\n",
    "        x = F.relu(torch.mul(x, w) - self.local_filter_bias)\n",
    "        return x\n",
    "\n",
    "    def get_adj(self, x, self_loop=True):\n",
    "        adj = torch.bmm(x, x.permute(0, 2, 1))\n",
    "        num_nodes = adj.shape[-1]\n",
    "        adj = F.relu(adj * (self.global_adj + self.global_adj.transpose(1, 0)))\n",
    "        if self_loop:\n",
    "            adj = adj + torch.eye(num_nodes).to(x.device)\n",
    "        rowsum = torch.sum(adj, dim=-1)\n",
    "        mask = torch.zeros_like(rowsum)\n",
    "        mask[rowsum == 0] = 1\n",
    "        rowsum += mask\n",
    "        d_inv_sqrt = torch.pow(rowsum, -0.5)\n",
    "        d_mat_inv_sqrt = torch.diag_embed(d_inv_sqrt)\n",
    "        adj = torch.bmm(torch.bmm(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)\n",
    "        return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77914873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmoothCrossEntropyLoss(Module):\n",
    "    def __init__(self, label_smoothing=0.0):\n",
    "        super(SmoothCrossEntropyLoss, self).__init__()\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if len(target.size()) == 1:\n",
    "            target = F.one_hot(target, num_classes=input.size(-1)).float().to(input.device)\n",
    "        \n",
    "        if self.label_smoothing > 0.0:\n",
    "            num_classes = input.size(-1)\n",
    "            smooth_value = self.label_smoothing / num_classes\n",
    "            target = target * (1.0 - self.label_smoothing) + smooth_value\n",
    "        \n",
    "        log_probs = F.log_softmax(input, dim=-1)\n",
    "        loss = -torch.sum(target * log_probs, dim=-1)\n",
    "\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def log_training_progress(epoch, batch, loss, accuracy, hyperparameters,log_file):\n",
    "    with open(log_file, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['Epoch', 'Batch', 'Loss', 'Accuracy', 'Hyperparameters']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        \n",
    "        if csvfile.tell() == 0:\n",
    "            writer.writeheader()\n",
    "\n",
    "        \n",
    "        writer.writerow({'Epoch': epoch, 'Batch': batch, 'Loss': loss, 'Accuracy': accuracy,\n",
    "                         'Hyperparameters': hyperparameters})\n",
    "\n",
    "def log_final_results(accuracy, loss, val_accuracy, val_precision, val_recall, val_f1, hyperparameters,log_file):\n",
    "    with open(log_file, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "        \n",
    "        writer.writerow(['Final Results'])\n",
    "        writer.writerow(['Accuracy', accuracy])\n",
    "        writer.writerow(['Loss', loss])\n",
    "        writer.writerow(['Validation Accuracy', val_accuracy])\n",
    "        writer.writerow(['Validation Precision', val_precision])\n",
    "        writer.writerow(['Validation recall', val_recall])\n",
    "        writer.writerow(['Validation F1', val_f1])\n",
    "        \n",
    "#         writer.writerow(['Validation Loss', val_loss])\n",
    "\n",
    "        \n",
    "        writer.writerow(['Hyperparameters'])\n",
    "        for key, value in hyperparameters.items():\n",
    "            writer.writerow([key, value])\n",
    "        writer.writerow(['-'*30])\n",
    "#         \n",
    "#         writer.writerow(['Model Summary'])\n",
    "#         writer.writerow([model_summary])\n",
    "\n",
    "def create_experiment_folder(experiment_folder):\n",
    "    os.makedirs(experiment_folder, exist_ok=True)\n",
    "    return experiment_folder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9229e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_set = TensorDataset(train_eeg_data, train_ecg_image, train_eda_image, train_dec_label, train_emo_label)\n",
    "train_loader = Data.DataLoader(train_set, batch_size=32)\n",
    "\n",
    "\n",
    "val_set = TensorDataset(val_eeg_data, val_ecg_image, val_eda_image, val_dec_label, val_emo_label)\n",
    "val_loader = DataLoader(val_set, batch_size=32)\n",
    "\n",
    "\n",
    "test_set = TensorDataset(test_eeg_data, test_ecg_image, test_eda_image, test_dec_label, test_emo_label)\n",
    "test_loader = DataLoader(test_set, batch_size=32)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c74a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm, trange\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, accuracy_score\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def multi_learning(alpha,beta,learning_rate1,epoch_size,dec_smoothing,emo_smoothing,iter_times):\n",
    "    # for i in range(len(model_name_list)):\n",
    "    #     model_name=model_name_list[i]\n",
    "\n",
    "    current_time = datetime.now()\n",
    "    \n",
    "    formatted_time = current_time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    print(formatted_time)\n",
    "    \n",
    "    experiment_folder = create_experiment_folder('experiment_result/'+formatted_time)\n",
    "    training_log_file = os.path.join(experiment_folder, 'training_log.csv')\n",
    "    final_results_log_file = os.path.join(experiment_folder, 'final_results_log.csv')\n",
    "\n",
    "    \n",
    "    \n",
    "    criterion1 = SmoothCrossEntropyLoss(label_smoothing=dec_smoothing)\n",
    "    criterion2 = SmoothCrossEntropyLoss(label_smoothing=emo_smoothing)\n",
    "\n",
    "#     learning_rate = 0.0005\n",
    "#     epoch_size = 50\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    ##############################\n",
    "    model_name = 'MMPS-EDNet'\n",
    "\n",
    "    save = True\n",
    "    \n",
    "    \n",
    "    hyperparameters={'alpha':alpha,'beta':beta,'epoch_size':epoch_size,'learning_rate': learning_rate1, 'batch_size': 32,\n",
    "                     'dec_smoothing':dec_smoothing,'emo_smoothing':emo_smoothing}\n",
    "    \n",
    "    \n",
    "    \n",
    "    acc_all = []\n",
    "    f1_all = []\n",
    "    recall_all = []\n",
    "    precision_all = []\n",
    "    \n",
    "    \n",
    "#     kf = KFold(n_splits=k_folder, shuffle=True)\n",
    "    \n",
    "    fold = 0\n",
    "    \n",
    "    for k in range(iter_times):\n",
    "        learning_rate=learning_rate1\n",
    "        fold += 1\n",
    "        print(f\"Fold {fold}\")\n",
    "        \n",
    "        \n",
    "        model = MMPS_EDNet(region_list=HEMISPHERE_REGION_LIST,chunk_size=875, num_electrodes=62, hid_channels=32, num_classes_dec=2,num_classes_emo=4).to(device)\n",
    "\n",
    "\n",
    "        \"\"\" Optimizer \"\"\"\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)#, weight_decay=0.0005\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        max_acc=0\n",
    "        for i in range(epoch_size):                                          #多个epoch\n",
    "            learning_rate=0.98*learning_rate\n",
    "            \n",
    "            loop = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "            model.train()\n",
    "\n",
    "\n",
    "            train_loss = 0.0\n",
    "            train_acc_task = 0.0\n",
    "            for step, (x_eeg,x_ecg,x_eda, y1, y2) in loop:\n",
    "                x_eeg, y1, y2 =  Variable(x_eeg).to(device), Variable(y1).to(device), Variable(y2).to(device)\n",
    "                x_ecg,x_eda = Variable(x_ecg).to(device), Variable(x_eda).to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                \n",
    "                pred_task, pred_emo,_ = model(x_eeg,x_ecg,x_eda)\n",
    "                loss1 = criterion1(pred_task, y1.long())\n",
    "                loss2 = criterion2(pred_emo,y2.long())\n",
    "                loss= alpha*loss1 + beta*loss2\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                pred_task = torch.max(pred_task, 1)[1]\n",
    "                train_correct_task = (pred_task == y1).sum()\n",
    "\n",
    "                train_acc_task += train_correct_task.item()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loop.set_description(f'Epoch [{i+1} / {epoch_size}]')\n",
    "                loop.set_postfix({\n",
    "                        'loss' : '{:.6f}'.format(train_loss/len(train_set)),\n",
    "                        'acc_task' : '{:.6f}'.format(train_acc_task*100/len(train_set))\n",
    "                                                    })\n",
    "                \n",
    "                log_training_progress(i, step, train_loss/len(train_set), train_acc_task*100/len(train_set), hyperparameters, training_log_file)\n",
    "#                 if i+1 == epoch_size and save == True:   \n",
    "#                     model_path = f'./model_Multi/%s_a=%f_b=%f_%d.pkl' % (model_name,alpha,beta,k+1)  #文件夹名称\n",
    "#                     os.makedirs('./model_Multi', exist_ok=True)   #创建文件夹\n",
    "#                     state = {'model':model.state_dict()\n",
    "#                             }\n",
    "#                     torch.save(state,model_path)\n",
    "                    \n",
    "            prob_all = []\n",
    "            label_all = []\n",
    "            with torch.no_grad():\n",
    "                model.eval()  \n",
    "                for x_eeg,x_ecg,x_eda, y1, y2 in val_loader:\n",
    "                    x_eeg, y1, y2 =  Variable(x_eeg).to(device), Variable(y1).to(device), Variable(y2).to(device)\n",
    "                    x_ecg,x_eda = Variable(x_ecg).to(device), Variable(x_eda).to(device)\n",
    "\n",
    "                    pred_task, pred_emo,_ = model(x_eeg,x_ecg,x_eda)\n",
    "                    prob = pred_task.cpu().numpy()\n",
    "                    prob_all.extend(np.argmax(prob,axis=1)) \n",
    "                    label_all.extend(y1.cpu().numpy())\n",
    "                acc_now = accuracy_score(y_true=prob_all, y_pred=label_all)\n",
    "                f1_now = f1_score(y_true=prob_all, y_pred=label_all, average=\"macro\")\n",
    "                recall_now = recall_score(y_true=prob_all, y_pred=label_all, average=\"macro\")\n",
    "                precision_now = precision_score(y_true=prob_all, y_pred=label_all, average=\"macro\")\n",
    "            if np.around(acc_now*100, 2)> max_acc:\n",
    "                max_acc=np.around(acc_now*100, 2)\n",
    "                max_precision=np.around(precision_now*100, 2)\n",
    "                max_recall=np.around(recall_now*100, 2)\n",
    "                max_f1=np.around(f1_now*100, 2)\n",
    "                #创建文件夹\n",
    "                model_folder = create_experiment_folder('./model_parameter/'+formatted_time)\n",
    "                model_path = f'./model_parameter/'+formatted_time+'/%s_fold=%d.pkl' % ('2d-image_multi',fold)  #文件夹名称\n",
    "                os.makedirs('./model_parameter', exist_ok=True)    \n",
    "                state = {'model':model.state_dict()}\n",
    "                \n",
    "                \n",
    "        log_final_results(train_loss/len(train_set), train_acc_task*100/len(train_set), max_acc, max_precision, max_recall,\n",
    "                              max_f1, hyperparameters, final_results_log_file)\n",
    "\n",
    "\n",
    "        print('accuracy:', max_acc)\n",
    "        print('precision:', max_precision)\n",
    "        print('recall:', max_recall)\n",
    "        print('f1:', max_f1)\n",
    "        torch.save(state,model_path)\n",
    "    \n",
    "\n",
    "    return formatted_time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe42333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_learning(1,0.85,learning_rate1=0.0002,epoch_size=100,dec_smoothing=0,emo_smoothing=0.17,iter_times=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST RESULT\n",
    "import joblib\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "acc_all = []\n",
    "f1_all = []\n",
    "recall_all = []\n",
    "precision_all = []\n",
    "\n",
    "fold = 0\n",
    "for k in range(5):\n",
    "    fold=fold+1\n",
    "    load_path = './model_parameter/2024-10-12_23-04-20/%s_fold=%d.pkl' % ('2d-image_multi',fold) \n",
    "    state_dict = torch.load(load_path)\n",
    "    model = MMPS_EDNet(region_list=HEMISPHERE_REGION_LIST,chunk_size=875, num_electrodes=62, hid_channels=32, num_classes_dec=2,num_classes_emo=4).to(device)\n",
    "\n",
    "    model.load_state_dict(state_dict['model'])\n",
    "    \n",
    "    test_set = TensorDataset(test_eeg_data, test_ecg_image, test_eda_image, test_dec_label, test_emo_label)\n",
    "    test_loader = Data.DataLoader(test_set, batch_size=32)\n",
    "\n",
    "\n",
    "\n",
    "    prob_all = []\n",
    "    label_all = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()  \n",
    "        for x_eeg,x_ecg,x_eda, y1, y2 in test_loader:\n",
    "            x_eeg, y1, y2 =  Variable(x_eeg).to(device), Variable(y1).to(device), Variable(y2).to(device)\n",
    "            x_ecg,x_eda = Variable(x_ecg).to(device), Variable(x_eda).to(device)\n",
    "\n",
    "            pred_task, pred_emo,_ = model(x_eeg,x_ecg,x_eda)\n",
    "            prob = pred_task.cpu().numpy()\n",
    "            prob_all.extend(np.argmax(prob,axis=1)) #求每一行的最大值索引\n",
    "            label_all.extend(y1.cpu().numpy())\n",
    "        acc_now = accuracy_score(y_true=prob_all, y_pred=label_all)\n",
    "        f1_now = f1_score(y_true=prob_all, y_pred=label_all, average=\"macro\")\n",
    "        recall_now = recall_score(y_true=prob_all, y_pred=label_all, average=\"macro\")\n",
    "        precision_now = precision_score(y_true=prob_all, y_pred=label_all, average=\"macro\")\n",
    "    acc_all.append(acc_now)\n",
    "    f1_all.append(f1_now)\n",
    "    recall_all.append(recall_now)\n",
    "    precision_all.append(precision_now)\n",
    "    print('accuracy:', np.around(acc_now*100, 2))\n",
    "    print('precision:', np.around(precision_now*100, 2))\n",
    "    print('recall:', np.around(recall_now*100, 2))\n",
    "    print('f1:', np.around(f1_now*100, 2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d71d350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3c94a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d93675a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde6c553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
